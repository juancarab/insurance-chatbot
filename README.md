# Insurance Chatbot

Demo API and web UI for the **Insurance Chatbot** project.

This monorepo includes:

- **Backend** (FastAPI) exposing the `/chat` endpoint.
- **Frontend** (Streamlit) with a chat UI consuming the API.
- **Agent** with web search tools (Tavily) and hybrid retrieval on **OpenSearch** (via Haystack).
- **Data pipelines** (EDA and ingestion into OpenSearch) and index setup utilities.
- Shared **`data/`** folder for PDFs, ingestion scripts, OpenSearch setup and evaluation assets.

---

## Requirements

- Python **3.10+**
- Docker and Docker Compose **v2**
- *(Optional – for real web search)* Tavily account and API key: https://app.tavily.com/home

---

## Repository structure (relevant)

```
.
├── docker-compose.yml
├── services/
│   ├── backend/
│   │   ├── app/              # FastAPI: main.py, config.py
│   │   └── Dockerfile
│   ├── frontend/
│   │   ├── app.py            # Streamlit UI
│   │   └── Dockerfile
│   └── agent/
│       └── app/
│           ├── langchain_runner.py
│           └── tools/
│               ├── retrieval/haystack_opensearch_tool.py
│               └── web_search/web_search.py
├── data/
│   ├── raw_policies/         # PDFs
│   ├── pipeline/             # ingest.py, eda_policies.py
│   ├── opensearch/setup_opensearch.py
│   └── test/test_opensearch_setup.py
├── tests/
└── web_search_cli.py
```

---

Environment variables

Create a .env file at the repo root (you can start from .env.example if it exists). Minimum example:

```env
# --- OpenSearch ---
# Docker Compose: host 'opensearch' | Local run: 'http://localhost:9200'
OPENSEARCH_HOST=http://localhost:9200
OPENSEARCH_PORT=9200
OPENSEARCH_INDEX=policies
OPENSEARCH_EMBED_DIM=384

# --- Web search (Tavily) ---
TAVILY_API_KEY=your_api_key
WEB_SEARCH_MAX_RESULTS=5
WEB_SEARCH_FRESHNESS_DAYS=30

# --- Backend formatter selection ---
# mock | gemini | langchain
INSURANCE_CHATBOT_FORMATTER=mock

# --- Gemini (if INSURANCE_CHATBOT_FORMATTER=gemini) ---
GEMINI_API_KEY=your_api_key
GEMINI_MODEL=gemini-2.5-flash
GEMINI_TEMPERATURE=0.2
GEMINI_TOP_P=0.95
GEMINI_MAX_OUTPUT_TOKENS=1024

# --- LangChain (if INSURANCE_CHATBOT_FORMATTER=langchain) ---
# With PYTHONPATH=services, the runner lives in services.agent.app.langchain_runner
INSURANCE_CHATBOT_LANGCHAIN_RUNNER=services.agent.app.langchain_runner:run_langchain_agent
```

> **Note:** When running the full stack with Docker Compose, OPENSEARCH_HOST must be opensearch (the service name). For direct local execution, use http://localhost:9200.

---

Quick start (Docker Compose – recommended)

1) **Build and start the stack**:
    ```bash
    docker compose up -d --build
    ```

2) **Create the hybrid OpenSearch index**:
    ```bash
    # Inside the backend container (recommended)
    docker compose exec backend bash -lc "python data/opensearch/setup_opensearch.py"
    
    # Alternative from your host (requires data/requirements installed):
    #   pip install -r data/requirements.txt
    #   python data/opensearch/setup_opensearch.py
    ```

3) **Ingest PDFs into OpenSearch (recommended):**
    ```bash
    docker compose exec backend bash -lc "python data/pipeline/ingest.py"
    ```

4) **Acces**:
    - **Backend**: <http://localhost:8000>  
      - Docs: <http://localhost:8000/docs>  
      - Health: <http://localhost:8000/health>
    - **Frontend**: <http://localhost:8501>
    ---

## Local run (without Docker Compose)

1) **Create and activate venv:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate        # Windows: .venv\Scripts\activate
    ```

2) **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3) **Start OpenSearch (using Docker is recommended):**
    ```bash
    docker compose up -d opensearch
    ```

4) **Create index and ingest policies:**
    ```bash
    python data/opensearch/setup_opensearch.py
    python data/pipeline/ingest.py
    ```

5) **Start the backend (from repo root):**
    ```bash
    PYTHONPATH=services uvicorn services.backend.app.main:app --reload --host 0.0.0.0 --port 8001
    ```

6) **Start the frontend (another terminal):** 
    ```bash
    export INSURANCE_CHATBOT_API_URL=http://127.0.0.1:8001/chat
    streamlit run services/frontend/app.py --server.port 8501
    ```
    - Enable the “Debug mode” checkbox in the sidebar to see agent steps (debug.steps) and retrieved sources in the UI.

---

## `/chat` endpoint contract

### Request
```json
{
  "messages": [
    {"role": "user", "content": "Latest user message"},
    {"role": "assistant", "content": "Optional previous assistant message"}
  ],
  "top_k": 3,
  "enable_web_search": false,
  "metadata": {"client": "web"}
}
```

- `messages`: ordered history; the last message must be from the user.
- `top_k`: max 10, number of retrieved chunks.
- `enable_web_search`: if `true` and `TAVILY_API_KEY` is set, real Tavily web search is used; otherwise, a stub is used.
- `metadata`: free-form metadata.

### Response
```json
{
  "answer": "Text generated by the formatter (mock/gemini/langchain).",
  "sources": [
    {
      "id": "policy-1",
      "title": "Policy document",
      "snippet": "Relevant excerpt...",
      "url": "https://example.com/policies/1"
    }
  ],
  "usage": {
    "retrieved_documents": 3,
    "web_search_enabled": false,
    "formatter": "langchain"
  }
}
```

- `answer`: final answer (mock / Gemini / LangChain agent).
- `sources`: traceability / justification contract.
- `usage`: diagnostic metrics (you can extend this).

### Example with `debug=true`

```bash
curl -s -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
        "messages": [
          {"role": "user", "content": "¿Qué cubre la póliza de hogar básica?"}
        ],
        "top_k": 3,
        "enable_web_search": false,
        "debug": true,
        "language": "es"
      }'
```

Example response (mock):

```json
{
  "answer": "(mock) Respondiendo en es. Cuando el LLM esté integrado...",
  "sources": [],
  "usage": {
    "retrieved_documents": 0,
    "web_search_enabled": false,
    "formatter": "langchain",
    "language": "es",
    "top_k": 3,
    "debug_enabled": true
  },
  "debug": {
    "formatter": "mock",
    "messages": [
      {"role": "user", "content": "¿Qué cubre la póliza de hogar básica?"}
    ],
    "contexts": [],
    "top_k": 3,
    "enable_web_search": false,
    "language": "es"
  }
}
```

With a real LangChain agent, the debug block includes graph steps, retrieved chunks, tools invoked, etc.

---

## Formatter selection (mock / gemini / langchain)

The backend behavior is controlled by `INSURANCE_CHATBOT_FORMATTER`:

### 1 Mock (default)
```env
INSURANCE_CHATBOT_FORMATTER=mock
```
Responds without calling external models.

### 2) Gemini
```env
INSURANCE_CHATBOT_FORMATTER=gemini
GEMINI_API_KEY=tu_api_key
GEMINI_MODEL=gemini-2.5-flash
```

### 3) LangChain + Tools (retrieval + web search)
```env
INSURANCE_CHATBOT_FORMATTER=langchain
INSURANCE_CHATBOT_LANGCHAIN_RUNNER=services.agent.app.langchain_runner:run_langchain_agent
TAVILY_API_KEY=tu_api_key             # para web search real
OPENSEARCH_HOST=opensearch|localhost  # según tu modo
```

The runner lives in `services/agent/app/langchain_runner.py` and can call:
- **Hybrid retrieval** (BM25 + embeddings) on OpenSearch:
  `services/agent/app/tools/retrieval/haystack_opensearch_tool.py`
- **Web search** (Tavily):  
  `services/agent/app/tools/web_search/web_search.py`

---

## OpenSearch (setup adn ingestion)

1) **Create hybrid index**:
    ```bash
    # Docker:
    docker compose exec backend bash -lc "python data/opensearch/setup_opensearch.py"
    
    # Local:
    python data/opensearch/setup_opensearch.py
    ```

2) **Ingest PDFs into policies index**:
    ```bash
    python data/pipeline/ingest.py
    ```

> The ingestion pipeline uses sentence-transformers/all-MiniLM-L6-v2 (dim=384), so `OPENSEARCH_EMBED_DIM` must be 384.

---

## Frontend (Streamlit)

- Entry point: `services/frontend/app.py`
- Default UI: <http://localhost:8501>.
- Configurable via env var: `INSURANCE_CHATBOT_API_URL` (by default it points to the local backend or to the `backend` service in Docker).

---

## Utilities & tests

- **Web search CLI (Tavily debugging):**
  ```bash
  python web_search_cli.py --q "qué cubre hospitalización" --k 5
  ```
- **Tests (OpenSearch setup, web search, etc.):**
  ```bash
  pytest -q
  # o:
  python -m pytest -q
  ```

### Golden Set evaluation (RAGAS)

1. Make sure the backend is running on `http://127.0.0.1:8001` (or adjust `--base-url`).
2. Run the evaluation script:
   ```bash
   python scripts/run_golden_set.py \
     --base-url http://127.0.0.1:8001/chat \
     --golden-set data/golden_set/golden_set.json \
     --output results/golden_before.jsonl
   ```
   - `data/golden_set/golden_set.json` includes 35 scenarios across five categories: `simple`, `follow_up`, `web`, `combined`, `negative`.
   - The script writes one row per question with the agent answer, timings and metadata. By default it saves to `results/golden_<timestamp>.jsonl`.
3. After implementing your changes, run again to generate `results/golden_after.jsonl`.
4. `*.metrics.json` file is also produced with RAGAS metrics (`faithfulness`, `context_precision`). Use `--baseline-metrics` to compare against a baseline:
   ```bash
   python scripts/run_golden_set.py \
     --base-url http://127.0.0.1:8001/chat \
     --baseline-metrics results/golden_before.metrics.json
   ```
5. By default the script uses `gemini-2.5-flash`. Make sure `GEMINI_API_KEY` is set (or pass `--gemini-api-key`):
   ```bash
   python scripts/run_golden_set.py \
     --base-url http://127.0.0.1:8001/chat \
     --ragas-model gemini-2.5-flash \
     --gemini-api-key "$GEMINI_API_KEY"
   ```
6. You can fill the `reference_answer` field in `data/golden_set/golden_set.json` to enrich manual review (currently only `faithfulness` and `context_precision` are computed).

---

## Troubleshooting

- **OpenSearch “unhealthy” or missing index**  
  Check ports `9200/9600`. 
  Run `setup_opensearch.py` and inspect logs:
  ```bash
  docker compose logs -f opensearch
  ```
- **Backend cannot import `backend.*` modules**  
  Run with `PYTHONPATH=services` as shown above.
- **Web search returns no results**  
  Check `TAVILY_API_KEY` and `enable_web_search=true` in your request; Verify the formatter/runner actually calls the web search tool.
- **Embedding dimension mismatch**  
  If you change the embedding model, update `OPENSEARCH_EMBED_DIM` and reindex.

---

## Windows / PowerShell quick commands (router & local debug)

- Integrate real retrieval by default in `/chat`.
- Refine prompts for the formatter (Gemini/LangChain).
- Improve traceability of sources and diagnostics in usage.

## Router multi-índice

- **Activate venv 3.9 and install
& .\.venv\Scripts\Activate.ps1
python --version   # debe decir 3.9.x
pip install -r requirements.txt
pip install eval-type-backport==0.2.2 importlib-metadata==6.8.0 "zipp>=3.15"

- **OpenSearch
docker compose up -d opensearch
curl.exe -s http://localhost:9200 | Out-String

- **PDFs ingestion → `policies` index
$env:OPENSEARCH_HOST = "http://localhost:9200"
python .\data\pipeline\ingest.py
curl.exe -s http://localhost:9200/policies/_count | Out-String  # must be > 0

- **Test policy locator (Stage 1)
$env:OPENSEARCH_HOST = "http://localhost:9200"
python -m services.agent.app.tools.find_relevant_policies "coaseguro en el extranjero"

- **Backend (choose one)
$env:PYTHONPATH = "services"

- **LangChain (use the real retriever)
$env:INSURANCE_CHATBOT_FORMATTER = "langchain"
$env:INSURANCE_CHATBOT_LANGCHAIN_RUNNER = "services.agent.app.langchain_runner:run_langchain_agent"
uvicorn services.backend.app.main:app --reload --host 127.0.0.1 --port 8001

- **Test requests
Invoke-RestMethod http://127.0.0.1:8001/health
'{"messages":[{"role":"user","content":"¿Cuál es el deducible anual de hospitalización?"}],"top_k":3,"enable_web_search":false,"debug":true,"language":"es"}' |
  Set-Content -Path req1.json -Encoding utf8 -NoNewline
curl.exe -s -X POST "http://127.0.0.1:8001/chat" -H "Content-Type: application/json; charset=utf-8" --data-binary "@req1.json"

